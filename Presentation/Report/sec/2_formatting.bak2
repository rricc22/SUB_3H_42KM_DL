\section{Data and Methodology}

\subsection{Dataset Processing}

We utilized the Endomondo dataset, initially containing 660,000 workouts with HR data. However, data quality was a major bottleneck given the crowdsourced nature of the data. To address this, we applied a rigorous pipeline consisting of 7 quality filters, including checks for valid sports types, complete HR data continuity, and high-fidelity GPS tracking.

This aggressive filtering was necessary to ensure model stability but significantly reduced the volume of data:
\begin{itemize}
	\item \textbf{Result:} Only 13,000 usable workouts remained, representing roughly 5\% of the total dataset.
\end{itemize}

To validate the consistency of this subset, we analyzed the data distribution across our Train, Validation, and Test splits.

\begin{figure}[htbp]
	\centering
	% Remplacez 'image_140d5a.png' par le nom réel de votre fichier si différent
	\includegraphics[width=\linewidth]{distribution_feature.png}
	\caption{Distribution of features (Speed, Altitude, Heart Rate) across Train, Validation, and Test sets. Note the normalized scales for input features versus the raw scale for the target Heart Rate.}
	\label{fig:data_distribution}
\end{figure}

As illustrated in Figure \ref{fig:data_distribution}, the distributions for Speed, Altitude, and Heart Rate are highly consistent across all three splits, minimizing the risk of covariate shift during evaluation. Notably, the input features for Speed and Altitude were normalized (centered around a mean of 0), whereas the Heart Rate target retains its original scale with a mean of approximately $149$ bpm.

Furthermore, we conducted a feature importance analysis to understand which variables drove the model's predictions.

\begin{figure}[htbp]
	\centering
	% Remplacez 'image_140d41.png' par le nom réel de votre fichier si différent
	\includegraphics[width=0.85\linewidth]{Feature_importance.png}
	\caption{Feature Importance Analysis showing the dominance of physiological history over kinematic metrics.}
	\label{fig:feature_importance}
\end{figure}

The analysis reveals a heavy reliance on physiological metrics over kinematic ones. Specifically:
\begin{itemize}
	\item \textbf{Dominant Features:} The mean heart rate (\texttt{hr\_mean}) is by far the most significant predictor (importance $> 0.5$), followed by heart rate standard deviation (\texttt{hr\_std}).
	\item \textbf{Kinematic Features:} Speed-related metrics (\texttt{speed\_std}, \texttt{speed\_mean}) play a secondary role.
	\item \textbf{Correlation Analysis:} Consistent with the feature importance chart, the direct normalized correlation between raw Speed and Heart Rate was found to be relatively low at $r=0.254$, reinforcing the need for non-linear modeling or aggregated statistical features to capture the relationship effectively.
\end{itemize}

To further justify our preprocessing strategy, we analyzed the impact of filtering and normalization on feature correlations.

\begin{figure}[htbp]
	\centering
	% IMAGE 1 : RAW VS PROCESSED
	\includegraphics[width=\linewidth]{processed.png}
	\caption{Comparison of correlations in Raw vs. Processed data. The rigorous filtering pipeline improved the Speed-Heart Rate correlation from $r=0.213$ to $r=0.254$ by removing noisy outliers.}
	\label{fig:raw_vs_processed}
\end{figure}

As shown in Figure \ref{fig:raw_vs_processed}, the raw data exhibited a weaker correlation ($r=0.213$) due to significant noise. Our filtering pipeline successfully refined this relationship to $r=0.254$, demonstrating that the "usable" 5\% of data contains a stronger, cleaner signal. Importantly, normalization preserved the statistical relationship ($r=0.254$ remained invariant), ensuring data integrity for model training.

\subsection{Critical Discovery: Weak Correlation Bottleneck}

The feature importance analysis revealed a fundamental limitation: the correlation between Speed and Heart Rate in the processed Endomondo dataset was only $r=0.254$. While preprocessing improved this from the raw data's $r=0.213$, this weak relationship severely constrains model performance.

\subsubsection{Root Cause Analysis}
We identified three primary factors contributing to the weak correlation:

\begin{enumerate}
	\item \textbf{Sparse HR Sampling:} Many Endomondo workouts contain interpolated HR data with only 0.4-1.0 measurements per minute, smoothing out the physiological response to speed changes.
	\item \textbf{Crowdsourced Noise:} Device heterogeneity (different GPS watches, phones) introduces measurement inconsistencies across workouts.
	\item \textbf{Population Heterogeneity:} Aggregating 13,855 workouts from diverse users with different fitness levels dilutes individual speed-to-HR patterns.
\end{enumerate}

\subsubsection{Hypothesis: High-Quality Data as Solution}
This discovery led to a critical hypothesis: \textbf{If we could obtain dense, high-quality HR data from a single source (e.g., Apple Watch), the speed-HR correlation should strengthen significantly, enabling better model predictions.}

This hypothesis motivated the transfer learning experiment described in Section 4.

\subsection{Model Architectures}
To effectively capture the temporal dependencies inherent in workout data, we experimented with three distinct architectures ranging from traditional recurrent networks to adapted large language models:

\begin{itemize}
	\item \textbf{LSTM (Long Short-Term Memory):} We implemented a standard LSTM architecture designed to mitigate the vanishing gradient problem in sequential data. The model consists of 2 layers with 64 hidden units each, resulting in a total of approximately 51,009 parameters.
	\item \textbf{GRU (Gated Recurrent Unit):} We tested a GRU-based model as a computationally efficient alternative to LSTM. GRUs simplify the gating mechanism, often achieving comparable performance with fewer parameters and faster training times.
	\item \textbf{Llama (Time-Series Adaptation):} We explored the capabilities of Large Language Models (LLMs) in the regression domain by adapting the Llama architecture. This experimental approach tests whether the attention mechanisms optimized for natural language can generalize to physiological time-series forecasting.
\end{itemize}
\subsection{Training Setup}
To ensure a fair comparison between these architectures, all models were trained using a standardized experimental setup. The specific hyperparameters and dataset splits used for LSTM, GRU, and Llama are detailed in Table \ref{tab:training_setup}.

\begin{table}[htbp]
	\centering
	\caption{Hyperparameters and Dataset Splits across all architectures}
	\label{tab:training_setup}
	\begin{tabular}{l c c c}
		\hline
		\textbf{Parameter} & \textbf{LSTM} & \textbf{GRU} & \textbf{Llama} \\
		\hline
		Epochs             & 100           & 100          & 100            \\
		Batch Size         & 16            & 16           & 16             \\
		Learning Rate      & 0.001         & 0.001        & 0.001          \\
		Train Samples      & 13,855        & 13,855       & 13,855         \\
		Validation Samples & 3,539         & 3,539        & 3,539          \\
		Test Samples       & 3,581         & 3,581        & 3,581          \\
		\hline
	\end{tabular}
\end{table}

Our experiments demonstrated that a batch size of \textbf{16} was optimal, yielding the lowest MAE of 13.88 BPM. We used MSE loss and Adam optimizer (LR=0.001).