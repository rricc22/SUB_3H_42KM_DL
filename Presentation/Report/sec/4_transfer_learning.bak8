\section{Transfer Learning: Validating the Data Quality Hypothesis}

\subsection{Motivation and Approach}

To test whether high-quality data could overcome the correlation bottleneck ($r=0.254$), we fine-tuned the pre-trained LSTM on 271 Apple Watch workouts (189 train/40 val/42 test) using a two-stage strategy: (1) freeze layers 0-2, train layer 3 + FC (LR=5e-4); (2) freeze layers 0-1, train layers 2-3 + FC (LR=1e-4). The critical improvement was HR sampling density (10-12 measurements/min vs. sparse), yielding 2.7× correlation improvement ($r=0.254 \rightarrow 0.68$).

\subsection{Results}

\begin{table}[htbp]
	\centering
	\caption{Transfer Learning Results}
	\label{tab:transfer_results}
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{Model}     & \textbf{Val MAE} & \textbf{Test MAE} & \textbf{R²}   \\
		\hline
		Endomondo Baseline & 13.88            & 13.64             & 0.44          \\
		Stage 1 Fine-Tuned & \textbf{9.61}    & \textbf{11.03}    & \textbf{0.59} \\
		\hline
		\multicolumn{4}{|c|}{\textbf{Improvement: -30.7\% MAE (validation)}}      \\
		\hline
	\end{tabular}
\end{table}

Stage 1 achieved validation MAE of 9.61 BPM, meeting the $<10$ BPM target. Stage 2 showed overfitting (12.70 BPM), confirming the need for conservative fine-tuning on small datasets.

\subsection{Key Findings}

\textbf{Correlation as performance ceiling.} Weak correlation ($r<0.3$) limits models to $\sim$13-14 BPM regardless of architecture, while strong correlation ($r>0.6$) enables $<10$ BPM.

\textbf{Data-efficient transfer learning.} Achieved target with only 189 samples (70× fewer than Endomondo) by preserving population knowledge in frozen layers while adapting top layers to individual patterns.

\textbf{Data quality dominates architecture.} High-quality data with fewer samples outperformed complex models on large noisy datasets.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.75\linewidth]{finetune_stage1_curves.png}
	\caption{Stage 1 training curves showing convergence at 9.61 BPM validation MAE after 31 epochs.}
	\label{fig:stage1_curves}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.75\linewidth]{finetune_predictions.png}
	\caption{Sample predictions on Apple Watch test set. The fine-tuned model captures HR dynamics more accurately than the baseline.}
	\label{fig:predictions}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.75\linewidth]{finetune_stage2_curves.png}
	\caption{Stage 2 fine-tuning attempt with unfrozen layers 2-3. Validation loss increased (overfitting), demonstrating the fragility of fine-tuning on small datasets and validating the conservative Stage 1 approach.}
	\label{fig:stage2_overfitting}
\end{figure}
