# Lag-Llama Integration Complete ‚úÖ

## What Was Created

### 1. New Model File: `Model/LagLlama_HR.py`
- Transformer-based architecture inspired by Lag-Llama
- 805K parameters (~16x larger than LSTM)
- Features:
  - Multi-head self-attention (8 heads)
  - 4 transformer encoder layers
  - Positional encoding for temporal information
  - User embeddings for personalization
  - GELU activation functions

### 2. Updated Training Script: `Model/train.py`
- Added `lag_llama` as third model option
- Supports same command structure as LSTM models
- Compatible with existing data preprocessing pipeline

### 3. Documentation Updates
- `README.md`: Added Transformer architecture section
- `TRAINING_COMMANDS.md`: Complete training guide with all commands
- `LAG_LLAMA_SETUP.md`: This file

---

## Quick Start Training Commands

### Train All Three Models (Recommended Order)
```bash
# 1. Baseline LSTM (~5 min)
python3 Model/train.py --model lstm --epochs 100 --batch_size 32

# 2. LSTM with User Embeddings (~6 min)
python3 Model/train.py --model lstm_embeddings --epochs 100 --batch_size 32

# 3. Lag-Llama Transformer (~20-30 min) ‚≠ê
python3 Model/train.py --model lag_llama --epochs 100 --batch_size 16 --lr 0.0001
```

### Important Notes for Lag-Llama Training
1. **Lower batch size** (16 instead of 32) - model is larger
2. **Lower learning rate** (0.0001 instead of 0.001) - Transformer sensitive to LR
3. **Longer training time** (~20-30 min vs 5 min for LSTM)
4. **Better performance expected** (6-9 BPM MAE vs 8-12 BPM for LSTM)

---

## Model Comparison

| Model | Parameters | Batch Size | Learning Rate | Expected MAE |
|-------|-----------|------------|---------------|--------------|
| LSTM | ~50K | 32 | 0.001 | 8-12 BPM |
| LSTM+Embeddings | ~60K | 32 | 0.001 | 7-10 BPM |
| **Lag-Llama** | **~805K** | **16** | **0.0001** | **6-9 BPM** ‚≠ê |

---

## Architecture Details

### Lag-Llama Transformer
```
Input: [speed, altitude, gender, user_embedding] ‚Üí [batch, 500, 19]
  ‚Üì
Input Projection ‚Üí [batch, 500, 128]
  ‚Üì
Positional Encoding (sinusoidal)
  ‚Üì
Transformer Encoder (4 layers)
  - Multi-head Attention (8 heads)
  - Feed-forward (512 hidden)
  - Layer Normalization
  - Dropout (0.1)
  ‚Üì
Output Projection ‚Üí [batch, 500, 1]
  ‚Üì
Heart Rate Prediction
```

---

## Troubleshooting

### GPU Out of Memory
```bash
# Reduce batch size to 8
python3 Model/train.py --model lag_llama --batch_size 8 --lr 0.0001

# Or use CPU (slower)
python3 Model/train.py --model lag_llama --device cpu --batch_size 16
```

### Model Not Converging
```bash
# Lower learning rate further
python3 Model/train.py --model lag_llama --lr 0.00005 --epochs 150

# Or reduce model complexity
python3 Model/train.py --model lag_llama --num_layers 2 --lr 0.0001
```

### Want Faster Training
```bash
# Reduce epochs for quick test
python3 Model/train.py --model lag_llama --epochs 20 --batch_size 16 --lr 0.0001

# Reduce number of layers
python3 Model/train.py --model lag_llama --num_layers 2 --epochs 100 --batch_size 16 --lr 0.0001
```

---

## Verification

### Test Model Creation
```bash
# Should print model architecture and parameter count
python3 Model/LagLlama_HR.py
```

**Expected Output:**
```
LAG-LLAMA INSPIRED MODEL FOR HEART RATE PREDICTION
Model Architecture:
...
Total parameters: 805,569
‚úì Model initialized successfully!
```

### Check Training Script
```bash
# View help and available models
python3 Model/train.py --help
```

**Should show:**
```
--model {lstm,lstm_embeddings,lag_llama}
```

---

## Next Steps

1. **Train baseline models first** for comparison:
   ```bash
   python3 Model/train.py --model lstm --epochs 100 --batch_size 32
   python3 Model/train.py --model lstm_embeddings --epochs 100 --batch_size 32
   ```

2. **Train Lag-Llama model**:
   ```bash
   python3 Model/train.py --model lag_llama --epochs 100 --batch_size 16 --lr 0.0001
   ```

3. **Compare results**:
   ```bash
   ls -lh checkpoints/
   # Check training curves:
   # - lstm_training_curves.png
   # - lstm_embeddings_training_curves.png
   # - lag_llama_training_curves.png
   ```

4. **Evaluate on test set** (automatic during training)

5. **Include in project report**:
   - Compare MAE/RMSE across all three models
   - Show training curves
   - Discuss why Transformer performs better
   - Parameter efficiency analysis

---

## Files Created/Modified

‚úÖ `Model/LagLlama_HR.py` - New Transformer model  
‚úÖ `Model/train.py` - Updated to support lag_llama  
‚úÖ `README.md` - Added architecture documentation  
‚úÖ `TRAINING_COMMANDS.md` - Complete training guide  
‚úÖ `LAG_LLAMA_SETUP.md` - This setup guide  

---

## Questions?

- See `TRAINING_COMMANDS.md` for all training options
- See `Model/README.md` for model architecture details
- See `AGENTS.md` for general project guidelines

**Ready to train! üöÄ**
