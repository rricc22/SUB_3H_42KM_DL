{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Preprocessed Data\n",
    "\n",
    "This notebook loads preprocessed tensors and reconstructs DataFrames to verify the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = '../DATA/processed'\n",
    "print(\"✓ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADED\n",
      "================================================================================\n",
      "\n",
      "Metadata: {'sequence_length': 500, 'num_train': 13855, 'num_val': 3539, 'num_test': 3581, 'random_seed': 42, 'version': 'streaming', 'formats': ['pt', 'h5']}\n",
      "\n",
      "Scaler params: {'speed_mean': 11.185626394859764, 'speed_std': 3.090054858423608, 'altitude_mean': 168.22054253312723, 'altitude_std': 489.72540129286784}\n",
      "\n",
      "Tensor keys: ['speed', 'altitude', 'heart_rate', 'timestamps', 'gender', 'userId', 'original_lengths']\n"
     ]
    }
   ],
   "source": [
    "# Load tensors\n",
    "train_data = torch.load(f'{DATA_DIR}/train.pt', weights_only=False)\n",
    "val_data = torch.load(f'{DATA_DIR}/val.pt', weights_only=False)\n",
    "test_data = torch.load(f'{DATA_DIR}/test.pt', weights_only=False)\n",
    "\n",
    "# Load metadata\n",
    "with open(f'{DATA_DIR}/metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "with open(f'{DATA_DIR}/scaler_params.json', 'r') as f:\n",
    "    scaler_params = json.load(f)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA LOADED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nMetadata: {metadata}\")\n",
    "print(f\"\\nScaler params: {scaler_params}\")\n",
    "print(f\"\\nTensor keys: {list(train_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Tensor Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSOR SHAPES:\n",
      "\n",
      "Train Set:\n",
      "  speed               : torch.Size([13855, 500, 1])    dtype=torch.float32\n",
      "  altitude            : torch.Size([13855, 500, 1])    dtype=torch.float32\n",
      "  heart_rate          : torch.Size([13855, 500, 1])    dtype=torch.float32\n",
      "  timestamps          : torch.Size([13855, 500, 1])    dtype=torch.float32\n",
      "  gender              : torch.Size([13855, 1])         dtype=torch.float32\n",
      "  userId              : torch.Size([13855, 1])         dtype=torch.int64\n",
      "  original_lengths    : torch.Size([13855, 1])         dtype=torch.int64\n",
      "\n",
      "Val Set:\n",
      "  speed               : torch.Size([3539, 500, 1])     dtype=torch.float32\n",
      "  altitude            : torch.Size([3539, 500, 1])     dtype=torch.float32\n",
      "  heart_rate          : torch.Size([3539, 500, 1])     dtype=torch.float32\n",
      "  timestamps          : torch.Size([3539, 500, 1])     dtype=torch.float32\n",
      "  gender              : torch.Size([3539, 1])          dtype=torch.float32\n",
      "  userId              : torch.Size([3539, 1])          dtype=torch.int64\n",
      "  original_lengths    : torch.Size([3539, 1])          dtype=torch.int64\n",
      "\n",
      "Test Set:\n",
      "  speed               : torch.Size([3581, 500, 1])     dtype=torch.float32\n",
      "  altitude            : torch.Size([3581, 500, 1])     dtype=torch.float32\n",
      "  heart_rate          : torch.Size([3581, 500, 1])     dtype=torch.float32\n",
      "  timestamps          : torch.Size([3581, 500, 1])     dtype=torch.float32\n",
      "  gender              : torch.Size([3581, 1])          dtype=torch.float32\n",
      "  userId              : torch.Size([3581, 1])          dtype=torch.int64\n",
      "  original_lengths    : torch.Size([3581, 1])          dtype=torch.int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TENSOR SHAPES:\\n\")\n",
    "for split_name, data in [(\"Train\", train_data), (\"Val\", val_data), (\"Test\", test_data)]:\n",
    "    print(f\"{split_name} Set:\")\n",
    "    for key, tensor in data.items():\n",
    "        print(f\"  {key:20s}: {str(tensor.shape):30s} dtype={tensor.dtype}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reconstruct DataFrame from Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame: (13855, 9)\n",
      "Val DataFrame:   (3539, 9)\n",
      "Test DataFrame:  (3581, 9)\n",
      "\n",
      "Columns: ['workout_id', 'userId', 'gender', 'original_length', 'seq_length', 'speed_seq', 'altitude_seq', 'heart_rate_seq', 'timestamp_seq']\n"
     ]
    }
   ],
   "source": [
    "def tensors_to_dataframe(data_dict):\n",
    "    \"\"\"\n",
    "    Reconstruct DataFrame from tensor dictionary - EXACTLY what's stored in tensors.\n",
    "    \n",
    "    Args:\n",
    "        data_dict: Dictionary with tensors (speed, altitude, heart_rate, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with one row per workout, showing ONLY tensor data (no computed stats)\n",
    "    \"\"\"\n",
    "    n_samples = len(data_dict['speed'])\n",
    "    seq_len = data_dict['speed'].shape[1]\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Extract ONLY what's stored in the tensors\n",
    "        record = {\n",
    "            'workout_id': i,\n",
    "            'userId': int(data_dict['userId'][i, 0]),\n",
    "            'gender': 'male' if data_dict['gender'][i, 0] == 1.0 else 'female',\n",
    "            'original_length': int(data_dict['original_lengths'][i, 0]),\n",
    "            'seq_length': seq_len,\n",
    "            # Store the actual sequences as lists (for inspection)\n",
    "            'speed_seq': data_dict['speed'][i, :, 0].numpy().tolist(),\n",
    "            'altitude_seq': data_dict['altitude'][i, :, 0].numpy().tolist(),\n",
    "            'heart_rate_seq': data_dict['heart_rate'][i, :, 0].numpy().tolist(),\n",
    "            'timestamp_seq': data_dict['timestamps'][i, :, 0].numpy().tolist()\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "# Reconstruct DataFrames - exactly what's in the tensors\n",
    "train_df = tensors_to_dataframe(train_data)\n",
    "val_df = tensors_to_dataframe(val_data)\n",
    "test_df = tensors_to_dataframe(test_data)\n",
    "print(f\"Train DataFrame: {train_df.shape}\")\n",
    "print(f\"Val DataFrame:   {val_df.shape}\")\n",
    "print(f\"Test DataFrame:  {test_df.shape}\")\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Reconstructed DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN DATAFRAME SAMPLE:\n",
      "   workout_id   userId gender  original_length  seq_length  \\\n",
      "0           0  2209215   male              344         500   \n",
      "1           1  2209215   male              331         500   \n",
      "2           2  2209215   male              370         500   \n",
      "3           3  2209215   male              383         500   \n",
      "4           4  2209215   male              326         500   \n",
      "5           5  2209215   male              499         500   \n",
      "6           6  2209215   male              183         500   \n",
      "7           7  4399772   male              500         500   \n",
      "8           8  4399772   male              500         500   \n",
      "9           9  4399772   male              500         500   \n",
      "\n",
      "                                           speed_seq  \\\n",
      "0  [-1.460224986076355, -1.051755428314209, -0.68...   \n",
      "1  [-0.8266516327857971, 0.12505881488323212, 0.7...   \n",
      "2  [-1.200984239578247, 0.2069382667541504, 1.181...   \n",
      "3  [0.45366236567497253, 0.4816609025001526, 0.68...   \n",
      "4  [-0.37924355268478394, 0.05587299168109894, 0....   \n",
      "5  [3.8261260986328125, 1.3960281610488892, 0.553...   \n",
      "6  [-1.351835012435913, 0.30657824873924255, 2.03...   \n",
      "7  [-2.3721346855163574, -1.1686608791351318, -0....   \n",
      "8  [-2.949988603591919, -2.9476585388183594, -2.8...   \n",
      "9  [-2.00398588180542, -0.16906702518463135, -0.5...   \n",
      "\n",
      "                                        altitude_seq  \\\n",
      "0  [-0.22936679422855377, -0.24801519513130188, -...   \n",
      "1  [-0.2941449284553528, -0.2941449284553528, -0....   \n",
      "2  [-0.2941449284553528, -0.2941449284553528, -0....   \n",
      "3  [-0.18716293573379517, -0.18618136644363403, -...   \n",
      "4  [-0.2725520133972168, -0.2715706527233124, -0....   \n",
      "5  [-0.3137749135494232, -0.3245709538459778, -0....   \n",
      "6  [-0.3216266632080078, -0.3245709538459778, -0....   \n",
      "7  [-0.3332899212837219, -0.3332899212837219, -0....   \n",
      "8  [-0.33492350578308105, -0.33492350578308105, -...   \n",
      "9  [1.9361451864242554, 1.9361451864242554, 1.936...   \n",
      "\n",
      "                                      heart_rate_seq  \\\n",
      "0  [65.0, 70.0, 75.0, 82.0, 94.0, 100.0, 100.0, 1...   \n",
      "1  [76.0, 76.0, 76.0, 81.0, 88.0, 94.0, 107.0, 11...   \n",
      "2  [73.0, 73.0, 73.0, 73.0, 70.0, 70.0, 71.0, 73....   \n",
      "3  [72.0, 72.0, 72.0, 72.0, 70.0, 71.0, 74.0, 78....   \n",
      "4  [110.0, 109.0, 104.0, 98.0, 92.0, 92.0, 96.0, ...   \n",
      "5  [88.0, 89.0, 92.0, 102.0, 100.0, 108.0, 109.0,...   \n",
      "6  [78.0, 84.0, 83.0, 90.0, 90.0, 90.0, 95.0, 100...   \n",
      "7  [76.0, 75.0, 75.0, 82.0, 88.0, 95.0, 102.0, 10...   \n",
      "8  [62.0, 62.0, 62.0, 62.0, 77.0, 81.0, 86.0, 94....   \n",
      "9  [66.0, 78.0, 87.0, 95.0, 101.0, 107.0, 108.0, ...   \n",
      "\n",
      "                                       timestamp_seq  \n",
      "0  [1331647232.0, 1331647232.0, 1331647232.0, 133...  \n",
      "1  [1267218816.0, 1267218816.0, 1267218816.0, 126...  \n",
      "2  [1266960128.0, 1266960128.0, 1266960128.0, 126...  \n",
      "3  [1266527488.0, 1266527488.0, 1266527488.0, 126...  \n",
      "4  [1256066176.0, 1256066176.0, 1256066176.0, 125...  \n",
      "5  [1211414272.0, 1211414272.0, 1211414272.0, 121...  \n",
      "6  [1211225088.0, 1211225088.0, 1211225088.0, 121...  \n",
      "7  [1420428800.0, 1420428800.0, 1420428800.0, 142...  \n",
      "8  [1413680768.0, 1413680768.0, 1413680768.0, 141...  \n",
      "9  [1394757376.0, 1394757376.0, 1394757376.0, 139...  \n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS:\n",
      "================================================================================\n",
      "         workout_id        userId  original_length  seq_length\n",
      "count  13855.000000  1.385500e+04     13855.000000     13855.0\n",
      "mean    6927.000000  5.389805e+06       410.236665       500.0\n",
      "std     3999.738325  4.238040e+06       137.540671         0.0\n",
      "min        0.000000  6.900000e+01        50.000000       500.0\n",
      "25%     3463.500000  1.958069e+06       340.000000       500.0\n",
      "50%     6927.000000  4.164701e+06       500.000000       500.0\n",
      "75%    10390.500000  8.632256e+06       500.000000       500.0\n",
      "max    13854.000000  1.527997e+07       500.000000       500.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTRAIN DATAFRAME SAMPLE:\")\n",
    "print(train_df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS:\")\n",
    "print(\"=\"*80)\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY CHECKS:\n",
      "\n",
      "Gender distribution:\n",
      "gender\n",
      "male      12384\n",
      "female     1471\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Original lengths: min=50, max=500, median=500\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'hr_min'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai-general/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'hr_min'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOriginal lengths: min=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[33m'\u001b[39m\u001b[33moriginal_length\u001b[39m\u001b[33m'\u001b[39m].min()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmax=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[33m'\u001b[39m\u001b[33moriginal_length\u001b[39m\u001b[33m'\u001b[39m].max()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, median=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[33m'\u001b[39m\u001b[33moriginal_length\u001b[39m\u001b[33m'\u001b[39m].median()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Check 3: HR range\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mHeart rate range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhr_min\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.min()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[33m'\u001b[39m\u001b[33mhr_max\u001b[39m\u001b[33m'\u001b[39m].max()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m BPM\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Check 4: Speed range\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpeed range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[33m'\u001b[39m\u001b[33mspeed_min\u001b[39m\u001b[33m'\u001b[39m].min()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[33m'\u001b[39m\u001b[33mspeed_max\u001b[39m\u001b[33m'\u001b[39m].max()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m km/h\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai-general/lib/python3.11/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai-general/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'hr_min'"
     ]
    }
   ],
   "source": [
    "print(\"DATA QUALITY CHECKS:\\n\")\n",
    "\n",
    "# Check 1: Gender distribution\n",
    "print(\"Gender distribution:\")\n",
    "print(train_df['gender'].value_counts())\n",
    "\n",
    "# Check 2: Sequence lengths\n",
    "print(f\"\\nOriginal lengths: min={train_df['original_length'].min()}, \"\n",
    "      f\"max={train_df['original_length'].max()}, median={train_df['original_length'].median():.0f}\")\n",
    "\n",
    "# Check 3: HR range\n",
    "print(f\"\\nHeart rate range: {train_df['hr_min'].min():.0f} - {train_df['hr_max'].max():.0f} BPM\")\n",
    "\n",
    "# Check 4: Speed range\n",
    "print(f\"Speed range: {train_df['speed_min'].min():.1f} - {train_df['speed_max'].max():.1f} km/h\")\n",
    "\n",
    "# Check 5: Correlations\n",
    "print(f\"\\nSpeed-HR correlation: mean={train_df['speed_hr_corr'].mean():.3f}, \"\n",
    "      f\"std={train_df['speed_hr_corr'].std():.3f}\")\n",
    "print(f\"Altitude-HR correlation: mean={train_df['altitude_hr_corr'].mean():.3f}, \"\n",
    "      f\"std={train_df['altitude_hr_corr'].std():.3f}\")\n",
    "\n",
    "print(\"\\n✓ Data quality verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify No Data Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users - Train: 428, Val: 92, Test: 92\n",
      "\n",
      "✅ NO DATA LEAKAGE: Users are properly split between train/val/test\n"
     ]
    }
   ],
   "source": [
    "# Check that users don't overlap between splits\n",
    "train_users = set(train_df['userId'].unique())\n",
    "val_users = set(val_df['userId'].unique())\n",
    "test_users = set(test_df['userId'].unique())\n",
    "\n",
    "print(f\"Unique users - Train: {len(train_users)}, Val: {len(val_users)}, Test: {len(test_users)}\")\n",
    "\n",
    "train_val_overlap = train_users & val_users\n",
    "train_test_overlap = train_users & test_users\n",
    "val_test_overlap = val_users & test_users\n",
    "\n",
    "if len(train_val_overlap) == 0 and len(train_test_overlap) == 0 and len(val_test_overlap) == 0:\n",
    "    print(\"\\n✅ NO DATA LEAKAGE: Users are properly split between train/val/test\")\n",
    "else:\n",
    "    print(f\"\\n❌ WARNING: Data leakage detected!\")\n",
    "    print(f\"  Train-Val overlap: {len(train_val_overlap)} users\")\n",
    "    print(f\"  Train-Test overlap: {len(train_test_overlap)} users\")\n",
    "    print(f\"  Val-Test overlap: {len(val_test_overlap)} users\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
