{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Preprocessed Data\n",
    "\n",
    "This notebook loads preprocessed tensors and reconstructs DataFrames to verify the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = '../DATA/processed'\n",
    "print(\"✓ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADED\n",
      "================================================================================\n",
      "\n",
      "Metadata: {'sequence_length': 500, 'num_train': 13855, 'num_val': 3539, 'num_test': 3581, 'random_seed': 42, 'version': 'streaming', 'formats': ['pt', 'h5']}\n",
      "\n",
      "Scaler params: {'speed_mean': 11.185626394859764, 'speed_std': 3.090054858423608, 'altitude_mean': 168.22054253312723, 'altitude_std': 489.72540129286784}\n",
      "\n",
      "Tensor keys: ['speed', 'altitude', 'heart_rate', 'timestamps', 'gender', 'userId', 'original_lengths']\n"
     ]
    }
   ],
   "source": [
    "# Load tensors\n",
    "train_data = torch.load(f'{DATA_DIR}/train.pt', weights_only=False)\n",
    "val_data = torch.load(f'{DATA_DIR}/val.pt', weights_only=False)\n",
    "test_data = torch.load(f'{DATA_DIR}/test.pt', weights_only=False)\n",
    "\n",
    "# Load metadata\n",
    "with open(f'{DATA_DIR}/metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "with open(f'{DATA_DIR}/scaler_params.json', 'r') as f:\n",
    "    scaler_params = json.load(f)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA LOADED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nMetadata: {metadata}\")\n",
    "print(f\"\\nScaler params: {scaler_params}\")\n",
    "print(f\"\\nTensor keys: {list(train_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Tensor Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSOR SHAPES:\n",
      "\n",
      "Train Set:\n",
      "  speed               : torch.Size([13855, 500, 1])    dtype=torch.float32\n",
      "  altitude            : torch.Size([13855, 500, 1])    dtype=torch.float32\n",
      "  heart_rate          : torch.Size([13855, 500, 1])    dtype=torch.float32\n",
      "  timestamps          : torch.Size([13855, 500, 1])    dtype=torch.float32\n",
      "  gender              : torch.Size([13855, 1])         dtype=torch.float32\n",
      "  userId              : torch.Size([13855, 1])         dtype=torch.int64\n",
      "  original_lengths    : torch.Size([13855, 1])         dtype=torch.int64\n",
      "\n",
      "Val Set:\n",
      "  speed               : torch.Size([3539, 500, 1])     dtype=torch.float32\n",
      "  altitude            : torch.Size([3539, 500, 1])     dtype=torch.float32\n",
      "  heart_rate          : torch.Size([3539, 500, 1])     dtype=torch.float32\n",
      "  timestamps          : torch.Size([3539, 500, 1])     dtype=torch.float32\n",
      "  gender              : torch.Size([3539, 1])          dtype=torch.float32\n",
      "  userId              : torch.Size([3539, 1])          dtype=torch.int64\n",
      "  original_lengths    : torch.Size([3539, 1])          dtype=torch.int64\n",
      "\n",
      "Test Set:\n",
      "  speed               : torch.Size([3581, 500, 1])     dtype=torch.float32\n",
      "  altitude            : torch.Size([3581, 500, 1])     dtype=torch.float32\n",
      "  heart_rate          : torch.Size([3581, 500, 1])     dtype=torch.float32\n",
      "  timestamps          : torch.Size([3581, 500, 1])     dtype=torch.float32\n",
      "  gender              : torch.Size([3581, 1])          dtype=torch.float32\n",
      "  userId              : torch.Size([3581, 1])          dtype=torch.int64\n",
      "  original_lengths    : torch.Size([3581, 1])          dtype=torch.int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TENSOR SHAPES:\\n\")\n",
    "for split_name, data in [(\"Train\", train_data), (\"Val\", val_data), (\"Test\", test_data)]:\n",
    "    print(f\"{split_name} Set:\")\n",
    "    for key, tensor in data.items():\n",
    "        print(f\"  {key:20s}: {str(tensor.shape):30s} dtype={tensor.dtype}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reconstruct DataFrame from Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame: (13855, 9)\n",
      "Val DataFrame:   (3539, 9)\n",
      "Test DataFrame:  (3581, 9)\n",
      "\n",
      "Columns: ['workout_id', 'userId', 'gender', 'original_length', 'seq_length', 'speed_seq', 'altitude_seq', 'heart_rate_seq', 'timestamp_seq']\n"
     ]
    }
   ],
   "source": [
    "def tensors_to_dataframe(data_dict):\n",
    "    \"\"\"\n",
    "    Reconstruct DataFrame from tensor dictionary - EXACTLY what's stored in tensors.\n",
    "    \n",
    "    Args:\n",
    "        data_dict: Dictionary with tensors (speed, altitude, heart_rate, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with one row per workout, showing ONLY tensor data (no computed stats)\n",
    "    \"\"\"\n",
    "    n_samples = len(data_dict['speed'])\n",
    "    seq_len = data_dict['speed'].shape[1]\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Extract ONLY what's stored in the tensors\n",
    "        record = {\n",
    "            'workout_id': i,\n",
    "            'userId': int(data_dict['userId'][i, 0]),\n",
    "            'gender': 'male' if data_dict['gender'][i, 0] == 1.0 else 'female',\n",
    "            'original_length': int(data_dict['original_lengths'][i, 0]),\n",
    "            'seq_length': seq_len,\n",
    "            # Store the actual sequences as lists (for inspection)\n",
    "            'speed_seq': data_dict['speed'][i, :, 0].numpy().tolist(),\n",
    "            'altitude_seq': data_dict['altitude'][i, :, 0].numpy().tolist(),\n",
    "            'heart_rate_seq': data_dict['heart_rate'][i, :, 0].numpy().tolist(),\n",
    "            'timestamp_seq': data_dict['timestamps'][i, :, 0].numpy().tolist()\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "# Reconstruct DataFrames - exactly what's in the tensors\n",
    "train_df = tensors_to_dataframe(train_data)\n",
    "val_df = tensors_to_dataframe(val_data)\n",
    "test_df = tensors_to_dataframe(test_data)\n",
    "print(f\"Train DataFrame: {train_df.shape}\")\n",
    "print(f\"Val DataFrame:   {val_df.shape}\")\n",
    "print(f\"Test DataFrame:  {test_df.shape}\")\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Reconstructed DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN DATAFRAME SAMPLE:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   workout_id  userId gender  original_length  seq_length  \\\n",
      "0           0  196571   male              500         500   \n",
      "1           1  196571   male              500         500   \n",
      "2           2  196571   male              500         500   \n",
      "3           3  196571   male              378         500   \n",
      "4           4  196571   male               83         500   \n",
      "5           5  196571   male              107         500   \n",
      "6           6  196571   male              130         500   \n",
      "7           7  196571   male              168         500   \n",
      "8           8  196571   male              107         500   \n",
      "9           9  196571   male              108         500   \n",
      "\n",
      "                                           speed_seq  \\\n",
      "0  [-2.2363219261169434, -1.8553011417388916, -1....   \n",
      "1  [-1.8416489362716675, 0.28313133120536804, 0.6...   \n",
      "2  [-2.2313573360443115, -1.710091233253479, -1.0...   \n",
      "3  [-1.3253471851348877, -0.6352900266647339, -0....   \n",
      "4  [-2.1953651905059814, -2.169301986694336, -1.4...   \n",
      "5  [-2.928861141204834, -2.318235158920288, -1.41...   \n",
      "6  [-1.4581458568572998, -0.3175658881664276, -0....   \n",
      "7  [-1.9620364904403687, -1.5921857357025146, -1....   \n",
      "8  [0.770887553691864, 0.42461785674095154, 0.602...   \n",
      "9  [-2.0861475467681885, -0.7941520810127258, 0.0...   \n",
      "\n",
      "                                        altitude_seq  \\\n",
      "0  [-0.44956424832344055, -0.44956424832344055, -...   \n",
      "1  [-0.06309667229652405, -0.07175816595554352, -...   \n",
      "2  [-0.36253687739372253, -0.3617119789123535, -0...   \n",
      "3  [-0.4425525665283203, -0.4425525665283203, -0....   \n",
      "4  [-0.4173929989337921, -0.4173929989337921, -0....   \n",
      "5  [-0.41368094086647034, -0.41368094086647034, -...   \n",
      "6  [-0.4371906816959381, -0.4371906816959381, -0....   \n",
      "7  [-0.4380156099796295, -0.4380156099796295, -0....   \n",
      "8  [-0.44461482763290405, -0.44461482763290405, -...   \n",
      "9  [-0.6599147915840149, -0.6599147915840149, -0....   \n",
      "\n",
      "                                      heart_rate_seq  \\\n",
      "0  [81.0, 85.0, 91.0, 97.0, 102.0, 110.0, 117.0, ...   \n",
      "1  [64.0, 71.0, 71.0, 76.0, 77.0, 87.0, 111.0, 12...   \n",
      "2  [71.0, 76.0, 81.0, 82.0, 87.0, 92.0, 103.0, 10...   \n",
      "3  [86.0, 91.0, 95.0, 100.0, 101.0, 105.0, 110.0,...   \n",
      "4  [109.0, 109.0, 111.0, 111.0, 112.0, 115.0, 115...   \n",
      "5  [65.0, 66.0, 72.0, 73.0, 79.0, 82.0, 89.0, 94....   \n",
      "6  [103.0, 108.0, 119.0, 119.0, 122.0, 127.0, 134...   \n",
      "7  [80.0, 87.0, 92.0, 99.0, 105.0, 110.0, 113.0, ...   \n",
      "8  [90.0, 90.0, 95.0, 100.0, 100.0, 106.0, 116.0,...   \n",
      "9  [65.0, 71.0, 78.0, 81.0, 82.0, 84.0, 84.0, 87....   \n",
      "\n",
      "                                       timestamp_seq  \n",
      "0  [1451250048.0, 1451250048.0, 1451250048.0, 145...  \n",
      "1  [1450742784.0, 1450742784.0, 1450742784.0, 145...  \n",
      "2  [1450626816.0, 1450626816.0, 1450626816.0, 145...  \n",
      "3  [1450138752.0, 1450138752.0, 1450138752.0, 145...  \n",
      "4  [1448138752.0, 1448138752.0, 1448138752.0, 144...  \n",
      "5  [1448134656.0, 1448134656.0, 1448134656.0, 144...  \n",
      "6  [1447894272.0, 1447894272.0, 1447894272.0, 144...  \n",
      "7  [1447891840.0, 1447891840.0, 1447891840.0, 144...  \n",
      "8  [1447889792.0, 1447889792.0, 1447889792.0, 144...  \n",
      "9  [1447888512.0, 1447888512.0, 1447888512.0, 144...  \n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS:\n",
      "================================================================================\n",
      "       workout_id        userId  original_length  seq_length\n",
      "count  679.000000  6.790000e+02       679.000000       679.0\n",
      "mean   339.000000  2.331069e+06       359.499264       500.0\n",
      "std    196.154701  3.150076e+06       167.166759         0.0\n",
      "min      0.000000  1.678600e+04        50.000000       500.0\n",
      "25%    169.500000  1.965710e+05       195.500000       500.0\n",
      "50%    339.000000  1.543833e+06       477.000000       500.0\n",
      "75%    508.500000  2.104631e+06       500.000000       500.0\n",
      "max    678.000000  1.369300e+07       500.000000       500.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTRAIN DATAFRAME SAMPLE:\")\n",
    "print(train_df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS:\")\n",
    "print(\"=\"*80)\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY CHECKS:\n",
      "\n",
      "Gender distribution:\n",
      "gender\n",
      "male      678\n",
      "female      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Original lengths: min=50, max=500, median=477\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'hr_min'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai-general/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'hr_min'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOriginal lengths: min=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[33m'\u001b[39m\u001b[33moriginal_length\u001b[39m\u001b[33m'\u001b[39m].min()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmax=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[33m'\u001b[39m\u001b[33moriginal_length\u001b[39m\u001b[33m'\u001b[39m].max()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, median=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[33m'\u001b[39m\u001b[33moriginal_length\u001b[39m\u001b[33m'\u001b[39m].median()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Check 3: HR range\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mHeart rate range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhr_min\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.min()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[33m'\u001b[39m\u001b[33mhr_max\u001b[39m\u001b[33m'\u001b[39m].max()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m BPM\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Check 4: Speed range\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpeed range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[33m'\u001b[39m\u001b[33mspeed_min\u001b[39m\u001b[33m'\u001b[39m].min()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[33m'\u001b[39m\u001b[33mspeed_max\u001b[39m\u001b[33m'\u001b[39m].max()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m km/h\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai-general/lib/python3.11/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai-general/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'hr_min'"
     ]
    }
   ],
   "source": [
    "print(\"DATA QUALITY CHECKS:\\n\")\n",
    "\n",
    "# Check 1: Gender distribution\n",
    "print(\"Gender distribution:\")\n",
    "print(train_df['gender'].value_counts())\n",
    "\n",
    "# Check 2: Sequence lengths\n",
    "print(f\"\\nOriginal lengths: min={train_df['original_length'].min()}, \"\n",
    "      f\"max={train_df['original_length'].max()}, median={train_df['original_length'].median():.0f}\")\n",
    "\n",
    "# Check 3: HR range\n",
    "print(f\"\\nHeart rate range: {train_df['hr_min'].min():.0f} - {train_df['hr_max'].max():.0f} BPM\")\n",
    "\n",
    "# Check 4: Speed range\n",
    "print(f\"Speed range: {train_df['speed_min'].min():.1f} - {train_df['speed_max'].max():.1f} km/h\")\n",
    "\n",
    "# Check 5: Correlations\n",
    "print(f\"\\nSpeed-HR correlation: mean={train_df['speed_hr_corr'].mean():.3f}, \"\n",
    "      f\"std={train_df['speed_hr_corr'].std():.3f}\")\n",
    "print(f\"Altitude-HR correlation: mean={train_df['altitude_hr_corr'].mean():.3f}, \"\n",
    "      f\"std={train_df['altitude_hr_corr'].std():.3f}\")\n",
    "\n",
    "print(\"\\n✓ Data quality verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify No Data Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users - Train: 18, Val: 4, Test: 5\n",
      "\n",
      "✅ NO DATA LEAKAGE: Users are properly split between train/val/test\n"
     ]
    }
   ],
   "source": [
    "# Check that users don't overlap between splits\n",
    "train_users = set(train_df['userId'].unique())\n",
    "val_users = set(val_df['userId'].unique())\n",
    "test_users = set(test_df['userId'].unique())\n",
    "\n",
    "print(f\"Unique users - Train: {len(train_users)}, Val: {len(val_users)}, Test: {len(test_users)}\")\n",
    "\n",
    "train_val_overlap = train_users & val_users\n",
    "train_test_overlap = train_users & test_users\n",
    "val_test_overlap = val_users & test_users\n",
    "\n",
    "if len(train_val_overlap) == 0 and len(train_test_overlap) == 0 and len(val_test_overlap) == 0:\n",
    "    print(\"\\n✅ NO DATA LEAKAGE: Users are properly split between train/val/test\")\n",
    "else:\n",
    "    print(f\"\\n❌ WARNING: Data leakage detected!\")\n",
    "    print(f\"  Train-Val overlap: {len(train_val_overlap)} users\")\n",
    "    print(f\"  Train-Test overlap: {len(train_test_overlap)} users\")\n",
    "    print(f\"  Val-Test overlap: {len(val_test_overlap)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
